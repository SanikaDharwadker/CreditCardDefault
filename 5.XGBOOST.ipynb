{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import Build_Evaluate_Model as bem\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test=bem.get_xy_traintest(scale=False,scaler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC XGBOOST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:22:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xg_score=bem.build_basic_model(X_train,y_train,X_test,y_test,classifier='XGBOOST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODEL</th>\n",
       "      <th>PARAMS</th>\n",
       "      <th>y_train_prob</th>\n",
       "      <th>y_test_prob</th>\n",
       "      <th>TRAIN SCORE</th>\n",
       "      <th>TEST SCORE</th>\n",
       "      <th>DIFFERENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>[0.10565758, 0.04975895, 0.16611354, 0.5044843...</td>\n",
       "      <td>[0.0322351, 0.18366416, 0.17870252, 0.5164707,...</td>\n",
       "      <td>0.952056</td>\n",
       "      <td>0.763209</td>\n",
       "      <td>0.188847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MODEL                                             PARAMS  \\\n",
       "0  Basic  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "\n",
       "                                        y_train_prob  \\\n",
       "0  [0.10565758, 0.04975895, 0.16611354, 0.5044843...   \n",
       "\n",
       "                                         y_test_prob  TRAIN SCORE  TEST SCORE  \\\n",
       "0  [0.0322351, 0.18366416, 0.17870252, 0.5164707,...     0.952056    0.763209   \n",
       "\n",
       "   DIFFERENCE  \n",
       "0    0.188847  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our Xgboost model is clearly overfitting .Let us see if we can reduce the overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WAYS TO CONTROL OVERFIT** \n",
    "-  max_depth default=6\n",
    "-  min_child_weigh -higher value more conservative the model\n",
    "-  gamma - min reduction in loss reqd to split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_model_1=xgb.XGBClassifier(max_depth=2,gamma=10,colsample_bytree=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:22:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xg_score=bem.build_model(X_train,y_train,X_test,y_test,classifier=xg_model_1,score_df=xg_score,classifier_name='xg_model_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODEL</th>\n",
       "      <th>PARAMS</th>\n",
       "      <th>y_train_prob</th>\n",
       "      <th>y_test_prob</th>\n",
       "      <th>TRAIN SCORE</th>\n",
       "      <th>TEST SCORE</th>\n",
       "      <th>DIFFERENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>[0.10565758, 0.04975895, 0.16611354, 0.5044843...</td>\n",
       "      <td>[0.0322351, 0.18366416, 0.17870252, 0.5164707,...</td>\n",
       "      <td>0.952056</td>\n",
       "      <td>0.763209</td>\n",
       "      <td>0.188847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xg_model_1</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>[0.14725907, 0.22128628, 0.2087531, 0.6687061,...</td>\n",
       "      <td>[0.20298962, 0.38469675, 0.22370285, 0.5019234...</td>\n",
       "      <td>0.810290</td>\n",
       "      <td>0.773939</td>\n",
       "      <td>0.036350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MODEL                                             PARAMS  \\\n",
       "0       Basic  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "1  xg_model_1  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "\n",
       "                                        y_train_prob  \\\n",
       "0  [0.10565758, 0.04975895, 0.16611354, 0.5044843...   \n",
       "1  [0.14725907, 0.22128628, 0.2087531, 0.6687061,...   \n",
       "\n",
       "                                         y_test_prob  TRAIN SCORE  TEST SCORE  \\\n",
       "0  [0.0322351, 0.18366416, 0.17870252, 0.5164707,...     0.952056    0.763209   \n",
       "1  [0.20298962, 0.38469675, 0.22370285, 0.5019234...     0.810290    0.773939   \n",
       "\n",
       "   DIFFERENCE  \n",
       "0    0.188847  \n",
       "1    0.036350  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPER-PARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   25.1s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   34.0s\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  50 | elapsed:   38.5s remaining:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  50 | elapsed:   40.9s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   41.0s finished\n",
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:23:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           random_state=0, reg_alpha=None,\n",
       "                                           reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.4, 0.5, 0.8],\n",
       "                                        'gamma': [5, 10, 15],\n",
       "                                        'max_depth': [1, 2, 3],\n",
       "                                        'n_estimators': [100, 200, 300]},\n",
       "                   scoring='roc_auc', verbose=10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier=xgb.XGBClassifier(random_state=0)\n",
    "param_grid={'colsample_bytree':[0.4,0.5,0.8],'n_estimators':[100,200,300],'gamma':[5,10,15],'max_depth':[1,2,3]}\n",
    "model=RandomizedSearchCV(estimator=classifier,param_distributions=param_grid,scoring='roc_auc',verbose=10,n_jobs=-1)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200, 'max_depth': 3, 'gamma': 5, 'colsample_bytree': 0.5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb=xgb.XGBClassifier(n_estimators= 200, max_depth= 3, gamma= 5, colsample_bytree= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:23:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xg_score=bem.build_model(X_train,y_train,X_test,y_test,classifier=best_xgb,score_df=xg_score,classifier_name='best_xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODEL</th>\n",
       "      <th>PARAMS</th>\n",
       "      <th>y_train_prob</th>\n",
       "      <th>y_test_prob</th>\n",
       "      <th>TRAIN SCORE</th>\n",
       "      <th>TEST SCORE</th>\n",
       "      <th>DIFFERENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>[0.10565758, 0.04975895, 0.16611354, 0.5044843...</td>\n",
       "      <td>[0.0322351, 0.18366416, 0.17870252, 0.5164707,...</td>\n",
       "      <td>0.952056</td>\n",
       "      <td>0.763209</td>\n",
       "      <td>0.188847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xg_model_1</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>[0.14725907, 0.22128628, 0.2087531, 0.6687061,...</td>\n",
       "      <td>[0.20298962, 0.38469675, 0.22370285, 0.5019234...</td>\n",
       "      <td>0.810290</td>\n",
       "      <td>0.773939</td>\n",
       "      <td>0.036350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best_xgb</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>[0.09483935, 0.12683137, 0.21216293, 0.6220972...</td>\n",
       "      <td>[0.041104645, 0.34025073, 0.23019384, 0.375729...</td>\n",
       "      <td>0.860262</td>\n",
       "      <td>0.775857</td>\n",
       "      <td>0.084405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MODEL                                             PARAMS  \\\n",
       "0       Basic  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "1  xg_model_1  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "2    best_xgb  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "\n",
       "                                        y_train_prob  \\\n",
       "0  [0.10565758, 0.04975895, 0.16611354, 0.5044843...   \n",
       "1  [0.14725907, 0.22128628, 0.2087531, 0.6687061,...   \n",
       "2  [0.09483935, 0.12683137, 0.21216293, 0.6220972...   \n",
       "\n",
       "                                         y_test_prob  TRAIN SCORE  TEST SCORE  \\\n",
       "0  [0.0322351, 0.18366416, 0.17870252, 0.5164707,...     0.952056    0.763209   \n",
       "1  [0.20298962, 0.38469675, 0.22370285, 0.5019234...     0.810290    0.773939   \n",
       "2  [0.041104645, 0.34025073, 0.23019384, 0.375729...     0.860262    0.775857   \n",
       "\n",
       "   DIFFERENCE  \n",
       "0    0.188847  \n",
       "1    0.036350  \n",
       "2    0.084405  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try and regularize best xgb model a bit more\n",
    "\n",
    "best_xgb_reg=xgb.XGBClassifier(n_estimators= 100, max_depth= 3, gamma= 15, colsample_bytree= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:24:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xg_score=bem.build_model(X_train,y_train,X_test,y_test,classifier=best_xgb_reg,score_df=xg_score,classifier_name='best_xgb_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODEL</th>\n",
       "      <th>PARAMS</th>\n",
       "      <th>y_train_prob</th>\n",
       "      <th>y_test_prob</th>\n",
       "      <th>TRAIN SCORE</th>\n",
       "      <th>TEST SCORE</th>\n",
       "      <th>DIFFERENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xg_model_1</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>[0.14725907, 0.22128628, 0.2087531, 0.6687061,...</td>\n",
       "      <td>[0.20298962, 0.38469675, 0.22370285, 0.5019234...</td>\n",
       "      <td>0.810290</td>\n",
       "      <td>0.773939</td>\n",
       "      <td>0.036350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>best_xgb_reg</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>[0.1516564, 0.2382438, 0.21785408, 0.6372192, ...</td>\n",
       "      <td>[0.13614623, 0.3620115, 0.27364364, 0.4818658,...</td>\n",
       "      <td>0.817803</td>\n",
       "      <td>0.775549</td>\n",
       "      <td>0.042254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best_xgb</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>[0.09483935, 0.12683137, 0.21216293, 0.6220972...</td>\n",
       "      <td>[0.041104645, 0.34025073, 0.23019384, 0.375729...</td>\n",
       "      <td>0.860262</td>\n",
       "      <td>0.775857</td>\n",
       "      <td>0.084405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>[0.10565758, 0.04975895, 0.16611354, 0.5044843...</td>\n",
       "      <td>[0.0322351, 0.18366416, 0.17870252, 0.5164707,...</td>\n",
       "      <td>0.952056</td>\n",
       "      <td>0.763209</td>\n",
       "      <td>0.188847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MODEL                                             PARAMS  \\\n",
       "1    xg_model_1  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "3  best_xgb_reg  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "2      best_xgb  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "0         Basic  XGBClassifier(base_score=0.5, booster='gbtree'...   \n",
       "\n",
       "                                        y_train_prob  \\\n",
       "1  [0.14725907, 0.22128628, 0.2087531, 0.6687061,...   \n",
       "3  [0.1516564, 0.2382438, 0.21785408, 0.6372192, ...   \n",
       "2  [0.09483935, 0.12683137, 0.21216293, 0.6220972...   \n",
       "0  [0.10565758, 0.04975895, 0.16611354, 0.5044843...   \n",
       "\n",
       "                                         y_test_prob  TRAIN SCORE  TEST SCORE  \\\n",
       "1  [0.20298962, 0.38469675, 0.22370285, 0.5019234...     0.810290    0.773939   \n",
       "3  [0.13614623, 0.3620115, 0.27364364, 0.4818658,...     0.817803    0.775549   \n",
       "2  [0.041104645, 0.34025073, 0.23019384, 0.375729...     0.860262    0.775857   \n",
       "0  [0.0322351, 0.18366416, 0.17870252, 0.5164707,...     0.952056    0.763209   \n",
       "\n",
       "   DIFFERENCE  \n",
       "1    0.036350  \n",
       "3    0.042254  \n",
       "2    0.084405  \n",
       "0    0.188847  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_score.sort_values(by='DIFFERENCE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Best Model`:xg_model_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
